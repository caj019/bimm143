---
title: "Class 9: Machine Learning pt1"
author: "Caitlin Johnson"
date: "2/4/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## K-means clustering
The main k-means function in R is called `kmeans()`. Let's play with it here
```{r}
# rnorm makes a random data distribution from a norm, and genearates a two column database with x/y coordinates centered around -3 and 3 respectively.
#cbind combines objects by column
# rev just reverses the order of a list, column, etc?
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```
Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results
Q. How many points are in each cluster? A: 30
Q. What ‘component’ of your result object details
 - cluster size? A: km$size
 - cluster assignment/membership? A: km$cluster
 - cluster center? km$centers
```{r}
?kmeans
km <- kmeans(x, centers = 2, nstart = 20)
km
km$size
km$centers
km$cluster
```
```{r}
km$size
```
```{r}
km$cluster
```
```{r}
length(km$cluster)
table(km$cluster)
```
```{r}
km$centers
```

Plot x colored by the kmeans cluster assignment and
 add cluster centers as blue points
```{r}
plot(x, col=km$cluster+1)
points(km$centers, col = 'blue')
#col = km$cluster is assigning a color to each cluster based on its cluster number
#The +2 is to get different colors, going through the color list?
# The points function allows us to color specific points a certain color.
```

##Hierarchical Clustering

The main Hierarchical Clustering function in R is called `hclust()`
An important point here is that you have to calculate the distance matrix deom your input data before calling `hclust()`


```{r}
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)
# The hclust() function returns a hierarchical
# clustering model
hc <- hclust(d = dist_matrix)
# the print method is not so useful here
hc 
```
```{r}
# Our input is a distance matrix from the dist()
# function. Lets make sure we understand it first
dist_matrix <- dist(x)
dim(dist_matrix)
dim(x)
dim(as.matrix(dist_matrix))
```

People often view the results of Hierarchical clustering graphically.
Let's try passing this to the `plot()` function
```{r}
plot(hc)
#This draws a dendrogram
#We can add aline to help visualize cutree
abline( h=6, col='red')
abline(h=4, col = 'blue')
#To get cluster membership vectors we need to 'cut' the tree at a certain height to yield our separate cluster branches
cutree(hc, h=6) #Cuts by height h
gp4 <- cutree(hc, h=4)
table(gp4)
```
```{r}
#We can use k to indicate how many groups we want
cutree(hc, k=4)
```

```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
x
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
#col groups
plot(hclust(dist(x)))
table(cutree(hclust(dist(x)), k = 2))
plot(x, col = cutree(hclust(dist(x)), k = 2))
#2 clusters
table(cutree(hclust(dist(x)), k = 3))
plot(x, col = cutree(hclust(dist(x)), k = 3))
# 3 clusters

```


Q. Use the dist(), hclust(), plot() and cutree()
 functions to return 2 and 3 clusters
 A: see above
Q. How does this compare to your known 'col' groups?
A: The one with 3 groups is pretty similar to 'col' groups, with only a few points differing
```{r}
#Idk what this is
hc <- hclust(dist(x))
plot(hc)

abline(h=2.7, col='blue')

abline(h=2.2, col= 'green')
```
To get cluster membership vector use `cutree()` and then use `table()` to tabulate up how many members in each cluster we have
```{r}
grps <- cutree(hc, k=3)
table(grps)
```
```{r}
plot(x, col = grps)
```



```{r}
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
plot(x)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)

```



##PCA of UK food Data
```{r}
x <- read.csv('UK_foods.csv', row.names = 1)
x
```
Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
```{r}
dim(x)
```
There are 17 rows and 4 columns. We can use `dim` to answer this question.
Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?
A: I prefer the second method (the method I used) since it's faster and less typing.

###Spotting major differences and trends
Lets make some plots to explore our data a bit more
```{r}
barplot(as.matrix(x), beside = T, col = rainbow(nrow(x)))
```
Q3: Changing what optional argument in the above barplot() function results in the following plot?
A: Changing beside = F gives the barplot below
```{r}
barplot(as.matrix(x), beside = F, col = rainbow(nrow(x)))
```
Q4 is non-existant
Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col= rainbow(10), pch =16)
?pairs
```
A: `pairs` makes a matrix of scatterplots comparing the data for different countries. For example, the scatterplot to the right of England and above Wales is comparing their consumption of different foods, with Wales on the x-axis and England on the y-axis. The next graph over to the right is comparing England and Scotland, with Scotland on the x-axis and England on the y-axis.

Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?
A: The number of fresh potatoes they consume per year
### Principal Component Analysis (PCA) with the `prcomp()` function
```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
?prcomp

```

What is in my result object `pca`? I can check the attributes!
```{r}
attributes(pca)
```

Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.
Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.
```{r}
plot(pca$x[,1], pca$x[,2], xlab = 'PC1', ylab = 'PC2', xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col = c('grey', 'red', 'blue', 'green'))
```
We can use the square of pca$sdev to calculate how much variation in the original data each PC accounts for
```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```
Or we can use `summary`
```{r}
z <- summary(pca)
z$importance
```
We can summarize the information in a plot of variance(eigenvectors) with respect to the principal component number (eigenvector  number)
```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

###Digging Deeper (variable loadings)
We can also consider the influence of each of the original variables upon the principal components (typically known as loading scores). This information can be obtained from the prcomp() returned $rotation component. It can also be summarized with a call to biplot().
```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )

```
Here we see observations (foods) with the largest positive loading scores that effectively “push” N. Ireland to right positive side of the plot (including Fresh_potatoes and Soft_drinks).
We can also see the observations/foods with high negative scores that push the other countries to the left side of the plot (including Fresh_fruit and Alcoholic_drinks).
Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?
```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )

```
A: If I did this correctly, the food groups most prominently featured are fresh potatoes and soft drinks again. 

###Biplots
Another way to see this information together with the main PCA plot is in a so-called biplot:
```{r}
## The inbuilt biplot() can be useful for small datasets 
biplot(pca)

```

##PCA of RNA-seq data
```{r}
rna.data <- read.csv('expression.csv', row.names = 1)
head(rna.data)
```
Q10: How many genes and samples are in this data set?
```{r}
dim(rna.data)
```
A: 100 genes and 10 samples for each.

Let's do a PCA and plot the results!
```{r}
## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE)
 
## Simple un ploished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2])
        
```
We can use the square of pca$sdev to calculate how much variation in the origial data each PC accounts for
```{r}
pca.var <- pca$sdev^2
#Precent variance is often more informative to look at
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```
We can use this to generate our standard scree-plot
```{r}
barplot(pca.var, main= 'Scree Plot', xlab = "Principal Component", ylab = "Percent Variation")
```
Now let's make our mian PCA plot a bit more attractive and useful...
```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca$x[,1], pca$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))

```
An easier approach to coloring our data as we did above:
```{r}
## Another way to color by sample type
## Extract the first 2 characters of the sample name
sample.type <- substr(colnames(rna.data),1,2)
sample.type
plot(pca$x[,1], pca$x[,2], col=as.factor(sample.type), pch=16)
```
#Find the top 10 measurements (genes) that contribute the most to pc1 in either direction (+ or -)
```{r}
loading_scores <- pca$rotation[,1]

## Find the top 10 measurements (genes) that contribute
## most to PC1 in either direction (+ or -)
gene_scores <- abs(loading_scores) 
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)

## show the names of the top 10 genes
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes 
```

```{r}
summary(pca)
```


kmeans(x, centers = , nstart =)
hclust(dist(x))
prcomp(t(x))