---
title: 'Class 10: Unsupervised Learning Mini-Project'
author: "Caitlin Johnson"
date: "2/6/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Questions: 
What exactly is a component? 
'$' refers to a specific column relative to a specific dataframe.
##1. Exploratory Data Analysis
```{r}
wisc.df <- read.csv('WisconsinCancer.csv')
# df for 'data frame'
```

```{r}
# Save your input data file to a new 'data' directory
fna.data <- "data/WisconsinCancer.csv"
head(wisc.df)
```
There are some funky things in this dataset that we will ignore for our analysis. This includes the first and second ID and Diagnosis columns and the funny last X column (col 33).

```{r}
# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[, 3:32])
#Set up row names of wisc.data
row.names(wisc.data) <- wisc.df$id
# Calling wisc.df$id makes the id column of wisc.df be the names of the rows for our wisc.data
#Create diagnosis vector for later
diagnosis <- wisc.df$diagnosis
wisc.data
diagnosis
```
Explore the data you created before (wisc.data and diagnosis) to answer the following questions: 
Q1. How many observations are in this dataset?
```{r}
nrow(wisc.data)
dim(wisc.data)
#Both give you the answer, but dim tells you how many columns there are too
```
A: There are 569 observations
Q2. How many of the observations have a malignant diagnosis?
```{r}
table(diagnosis)
length(grep('M', diagnosis))
#Both of these tell you the same result
```
A: There are 212 malignant diagnosis
Q3. How many variables/features in the data are suffixed with _mean?

```{r}
grep('mean', colnames(wisc.data))
grep('mean', colnames(wisc.data), value = T)

length(grep('mean', colnames(wisc.data)))

```
A: 10

##2. Pricipal Component Analysis
###Performing PCA

Before we turn to PCA, we need to think, or consider, whether we should SCALE our input. 
```{r}
# Check column means and standard deviations
colMeans(wisc.data)

round(apply(wisc.data,2,sd), 2)
#Calling 'round' rounds up our data, of course, but it also allows us to see the data in 'normal' notation as opposed to scientific notation
```

Looks like we need to set scale=TRUE.
```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data, scale = T)
summary(wisc.pr)
```
Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
A: 0.4427
Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
A: 3 (PC1-PC3)

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
A: 7 (PC1 -PC7)


### Interpreting PCA
Let's make some figures

```{r}
biplot(wisc.pr)
```
Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
A: All the vectors begin at the same point. This plot is very difficult to understand, because all the data is on top of each other and is nearly impossible to read!
```{r}
attributes(wisc.pr)
# x is the scores of our original.....????
# I think it's the x coordinate 
```
Question: What does the $x do???
- Also, what are we calling when we do wisc.pr$x[ ,1]?
```{r}
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[ ,1], wisc.pr$x[ ,2], col = wisc.df$diagnosis, xlab = "PC1", ylab = "PC2")
#Black in benign, red is malignant
abline(v=0, col='gray', lty = 2)
abline(h=0, col='gray', lty=2)
```

Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[ ,1], wisc.pr$x[ ,3], col = wisc.df$diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

### Variance Explained 
```{r}
# Calculate variance of each component by squaring the sdev component of wisc.pr
pr.var <- wisc.pr$sdev^2
head(pr.var)
# head(pr.var) is looking at just the beginning of our pr.var, so only the first few components (PCs)

```
Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called pve and create a plot of variance explained for each principal component.

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
pve

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

Optional: There are quite a few CRAN packages that are helpful for PCA. This includes the factoextra package. Feel free to explore this package. For example:
```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
```{r}
wisc.pr$rotation[,1]
```

A: -0.26085376 

Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
A: 5

##3. Hierarchical Clustering
```{r}
#Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)

```
Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to data.dist.
```{r}
data.dist <- dist(data.scaled)
```
Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to wisc.hclust.
```{r}
wisc.hclust <- hclust(data.dist, method = 'complete')
```
Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
```{r}
plot(wisc.hclust)
abline(h = 20, col = 'red', lty = 2)
```
A: A little below 20. 
Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```
Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?
A: 9 and 10 seem to be good?

##4. OPTIONAL: K-means clustering
Create a k-means model on wisc.data, assigning the result to wisc.km.
Use the table() function to compare the cluster membership of the k-means model (wisc.km$cluster) to the actual diagnoses contained in the diagnosis vector.
```{r}
wisc.km <- kmeans(wisc.data, centers= 2, nstart= 20)
table(wisc.km$cluster)
table(diagnosis)
table(wisc.km$cluster, diagnosis)
```
Q13. How well does k-means separate the two diagnoses? How does it compare to your hclust results?
It seems to be a bit too specific? It didn't give enough malignant dianosis. It seems less accurate than the hclust results.
Use the table() function to compare the cluster membership of the k-means model (wisc.km$cluster) to your hierarchical clustering model from above (wisc.hclust.clusters).
```{r}
table(wisc.km$cluster, wisc.hclust.clusters)
```
Looking at the second table you generated, it looks like clusters 1, 2, and 4 from the hierarchical clustering model can be interpreted as the cluster 1 equivalent from the k-means algorithm, and cluster 3 can be interpreted as the cluster 2 equivalent.

##5. Combining methods
###Cluster on PCA results *DONE EXCEPT FOR OPTIONAL 3D FIGURE

First lets see if we can cluster the original data
```{r}
wisc.hc <- hclust(dist(wisc.data))
plot(wisc.hc)
```
This looks awful! Instead of doing hclust on the raw data (wisc.data), what if we did it on the PCA results?

Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Wardâ€™s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:3]), method = 'ward.D2')
plot(wisc.pr.hclust)

```

This dendrogram is really hard to read, just because there are so many patients. Let's use cutree to see how many patients are in a certain number of groups (membership vector).
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

We can also see the number of diagnosis in each group!
```{r}
table(grps, diagnosis)
```
Let's plot our clusters!
```{r}
plot(wisc.pr$x[,1:2], col = grps)
```
We can use the `table()` function to compare the $diagnosis vector with out cluster results vector.
```{r}
table(grps, wisc.df$diagnosis)
```
All the chunk of code down below does is reorder the colors so they match the colors from earlier, so red is malignant. 
```{r}
g <- as.factor(grps)
levels(g)
g <- relevel(g, 2)
levels(g)
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```


```{r eval=F}
#Not working
install.packages('rgl')
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
```

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
#Cut this hierarchical clustering model into 2 clusters and assign the results to wisc.pr.hclust.clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```
Q14. How well does the newly created model with four clusters separate out the two diagnoses?
```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters)
table(diagnosis)
table(wisc.pr.hclust.clusters, diagnosis)
```
A: Kinda confused on what this is saying, but I think it's saying that the wisc.pr.hclust.clusters successfully diagnosed 329 people as benign, succesfully diagnosed 188 people as malignant, and gave 28 false negatives and 24 false positives?

Q15. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.
```{r}
table(wisc.hclust.clusters, diagnosis)
#As of right now, I can't compare with km because I haven't done that section
table(wisc.hclust.clusters)
#table(___, diagnosis)
```
A: Now I really don't know what this is saying. Actually, I think it's saying it gave 343 correct benign diagnoses, 165 correct malignant diagnoses, 14 false positives, and 47 false negatives. 

##6.Sensitivity/Specificity
Q16. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?
```{r}
#Sensitivity
#wisc.pr.hclust.clusters
216/212
#wisc.hclust.clusters
(177+7)/(212)
#Specificity
#wisc.pr.hclust.clusters
(353/357)
#wisc.hclust.clusters
(383+4)/357
```
A: The principal components method gave higher sensitivity, and the hierarchical clustering method gave higher selectivity. 

##7.Prediction

We will use the predict() function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space.

```{r}
new <- read.csv('new_samples.csv')
#Use the predict function with our previous PCA model and new data...
npc <- predict(wisc.pr, newdata=new)
npc
```
Now draw the PCA plot again and add our new data

```{r}
plot(wisc.pr$x[,1:2], col=wisc.df$diagnosis)
points(npc[,1], npc[,2], col="blue", pch=15, cex=3)
text(npc[,1], npc[,2], labels = c(1,2), col="white")
```
Q17. Which of these new patients should we prioritize for follow up based on your results?
A: Patient 2. Idk tho.

